from flask import Flask, render_template, request, jsonify
import re

app = Flask(__name__)

def sql_to_drizzle_advanced(sql_text: str) -> tuple[str, str]:
    """
    1. Generates Drizzle ORM (pg-core) TypeScript for a single CREATE TABLE.
    2. Auto-casts created_at | updated_at | deleted_at → timestamp().
    3. Detects “ON UPDATE CURRENT_TIMESTAMP” on updated_at →
       • adds .$onUpdate(() => sql`CURRENT_TIMESTAMP`) to the column
       • returns PL/pgSQL trigger DDL that keeps the column fresh in-database.

    Returns (drizzle_schema_ts, trigger_sql)  – trigger_sql is '' when not needed.
    """

    # ── normalise ───────────────────────────────────────────────────────────────
    sql_text = (sql_text or '').strip()
    sql_text = re.sub(r'`', '', sql_text)          # drop MySQL back-ticks
    sql_text = re.sub(r'\s+', ' ', sql_text)       # squeeze whitespace

    # ── table name ─────────────────────────────────────────────────────────────
    m_table = re.search(
        r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?([A-Za-z_]\w*)',
        sql_text, re.IGNORECASE)
    table_name = m_table.group(1) if m_table else 'unknown'

    # ── isolate (…) block ───────────────────────────────────────────────────────
    open_idx = sql_text.find('(')
    if open_idx == -1:
        return f'// Failed to parse table {table_name}', ''
    depth = 0
    close_idx = -1
    for i, ch in enumerate(sql_text[open_idx:], start=open_idx):
        if ch == '(': depth += 1
        elif ch == ')':
            depth -= 1
            if depth == 0:
                close_idx = i
                break
    if close_idx == -1:
        return f'// Incomplete column block for {table_name}', ''

    inner = sql_text[open_idx + 1:close_idx].strip()
    items = [s.strip() for s in re.split(r',(?![^()]*\))', inner) if s.strip()]

    # ── working structures ─────────────────────────────────────────────────────
    columns: dict[str, dict] = {}
    enums, table_pk, table_fks = [], [], []
    triggers_needed, upd_cols = False, []

    def parse_actions(tail: str):
        od = ou = None
        m = re.search(r'ON\s+DELETE\s+(CASCADE|SET\s+NULL|RESTRICT|NO\s+ACTION)', tail, re.I)
        if m: od = m.group(1).lower().replace(' ', '') or None
        m = re.search(r'ON\s+UPDATE\s+(CASCADE|SET\s+NULL|RESTRICT|NO\s+ACTION)', tail, re.I)
        if m: ou = m.group(1).lower().replace(' ', '') or None
        if od == 'noaction': od = 'no action'
        if ou == 'noaction': ou = 'no action'
        return od, ou

    # ── pass 1: parse each line ─────────────────────────────────────────────────
    for raw in items:
        upper = raw.upper()

        # table-level PK
        if (m := re.match(r'PRIMARY\s+KEY\s*\(([^)]+)\)', upper, re.I)):
            table_pk = [c.strip() for c in m.group(1).split(',')]
            continue

        # table-level FK
        m = re.match(
            r'FOREIGN\s+KEY\s*\(([^)]+)\)\s*REFERENCES\s+([A-Za-z_]\w*)\s*\(([^)]+)\)\s*(.*)$',
            raw, re.I)
        if m:
            cols = [c.strip() for c in m[1].split(',')]
            ref_table, ref_cols = m[2], [c.strip() for c in m[3].split(',')]
            od, ou = parse_actions(m[4])
            table_fks.append({'cols': cols, 'ref_table': ref_table,
                              'ref_cols': ref_cols, 'onDelete': od, 'onUpdate': ou})
            continue

        # column line
        name, *rest_parts = raw.split(None, 1)
        rest = rest_parts[0] if rest_parts else ''
        rest_up = rest.upper()
        col = {
            'name': name,
            'ts_type': '',
            'length': None,
            'not_null': 'NOT NULL' in rest_up,
            'default': None,
            'primary': 'PRIMARY KEY' in rest_up,
            'identity': any(tok in rest_up for tok in
                            ('AUTO_INCREMENT', 'GENERATED ALWAYS AS IDENTITY',
                             'GENERATED BY DEFAULT AS IDENTITY')),
            'enum_name': None,
            'references': None,
            'raw_type': '',
        }

        # ENUM → generate pgEnum & mark type
        if (m := re.search(r'ENUM\s*\(([^)]*)\)', rest, re.I)):
            vals = [v.strip().strip("'").strip('"') for v in
                    re.split(r',(?![^()]*\))', m[1]) if v.strip()]
            enum_name = f'{name}Enum'
            enums.append(f"export const {enum_name} = pgEnum('{name}', {vals});")
            col.update(ts_type='enum', enum_name=enum_name, raw_type='ENUM')

        # REFERENCES
        if (m := re.search(r'REFERENCES\s+([A-Za-z_]\w*)\s*\(\s*([A-Za-z_]\w*)\s*\)\s*(.*)$',
                           rest, re.I)):
            od, ou = parse_actions(m[3])
            col['references'] = {'table': m[1], 'column': m[2],
                                 'onDelete': od, 'onUpdate': ou}

        # DEFAULT literal/function
        for pat, conv in [
            (r"DEFAULT\s+'([^']*)'", lambda g: f'"{g}"'),
            (r'DEFAULT\s+"([^"]*)"', lambda g: f'"{g}"'),
            (r'DEFAULT\s+([+-]?\d+(?:\.\d+)?)', lambda g: g),
            (r'DEFAULT\s+(true|false|TRUE|FALSE|0|1)', lambda g:
                'true' if g.lower() in ('1', 'true') else 'false'),
            (r'DEFAULT\s+([A-Za-z_]\w*\([^)]*\))', lambda g: g),
        ]:
            if (m := re.search(pat, rest, re.I)):
                col['default'] = conv(m[1])
                break

        # AUDIT columns: force timestamp
        audit = name.lower() in ('created_at', 'updated_at', 'deleted_at')
        if audit:
            col['ts_type'] = 'timestamp'
            if name.lower() == 'updated_at' and 'ON UPDATE CURRENT_TIMESTAMP' in rest_up:
                triggers_needed, upd_cols = True, ['updated_at']

        # explicit type mapping if not yet decided
        if not col['ts_type'] and col['raw_type'] != 'ENUM':
            tmap = [('integer', r'\b(BIGINT|SMALLINT|INT|INTEGER)\b', 'INT'),
                    ('varchar', r'(VARCHAR|CHAR)\b', 'VARCHAR'),
                    ('text', r'(LONGTEXT|TEXT)\b', 'TEXT'),
                    ('double', r'(DOUBLE|FLOAT|DECIMAL|NUMERIC)\b', 'DOUBLE'),
                    ('boolean', r'BOOLEAN|TINYINT\(1\)', 'BOOLEAN'),
                    ('timestamp', r'(DATETIME|TIMESTAMP|DATE)\b', 'TIMESTAMP')]
            for t, pat, raw_t in tmap:
                if re.search(pat, rest_up):
                    col['ts_type'], col['raw_type'] = t, raw_t
                    if t == 'varchar' and (m := re.search(r'\((\d+)\)', rest)):
                        col['length'] = int(m[1])
                    break
        # inference rules
        if col['identity']:
            col.update(ts_type='integer', primary=True)
        if col['primary'] and not col['ts_type']:
            col['ts_type'] = 'integer'
        if not col['ts_type']:
            col['ts_type'] = 'text'

        columns[name] = col

    # ── pass 2: table-level PK inference ────────────────────────────────────────
    composite_pk = None
    if table_pk:
        if len(table_pk) == 1:
            pk = table_pk[0]
            if pk in columns:
                columns[pk]['primary'] = True
                if not columns[pk]['raw_type']:
                    columns[pk].update(ts_type='varchar', length=200)
        else:
            composite_pk = [c for c in table_pk if c in columns]
            for pk in composite_pk:
                if not columns[pk]['raw_type']:
                    columns[pk].update(ts_type='varchar', length=200)

    # ── render Drizzle TS ───────────────────────────────────────────────────────
    out = [*enums, f'export const {table_name} = pgTable("{table_name}", {{']

    def builder(c):
        parts = [
            f'  {c["name"]}: ',
            f'{c["enum_name"]}("{c["name"]}")' if c['ts_type'] == 'enum' else
            f'{c["ts_type"]}("{c["name"]}")' if c["ts_type"] != "varchar" else
            f'varchar("{c["name"]}", {{ length: {c["length"] or 255} }})'
        ]
        if c['default'] is not None: parts.append(f'.default({c["default"]})')
        if c['not_null']: parts.append('.notNull()')
        if c['primary']:  parts.append('.primaryKey()')
        if c['identity']: parts.append('.generatedAlwaysAsIdentity()')
        if c['references']:
            opts = []
            if c['references']['onDelete']: opts.append(f"onDelete: '{c['references']['onDelete']}'")
            if c['references']['onUpdate']: opts.append(f"onUpdate: '{c['references']['onUpdate']}'")
            opt = f", {{ {', '.join(opts)} }}" if opts else ''
            parts.append(f'.references(() => {c["references"]["table"]}.{c["references"]["column"]}{opt})')
        if c['name'] == 'updated_at' and triggers_needed:
            parts.append('.$onUpdate(() => sql`CURRENT_TIMESTAMP`)')
        parts.append(',')
        return ''.join(parts)

    out += [builder(col) for col in columns.values()]
    out.append('}',)

    # table-level composite PK / FKs
    builders = []
    if composite_pk:
        cols = ', '.join(f'table.{c}' for c in composite_pk)
        builders.append(f'pk: primaryKey({cols})')
    for fk in table_fks:
        if len(fk['cols']) == 1 and not columns[fk['cols'][0]].get('references'):
            columns[fk['cols'][0]]['references'] = {
                'table': fk['ref_table'], 'column': fk['ref_cols'][0],
                'onDelete': fk['onDelete'], 'onUpdate': fk['onUpdate']}
        else:
            cols = ', '.join(f'table.{c}' for c in fk["cols"])
            refs = ', '.join(f'{fk["ref_table"]}.{r}' for r in fk["ref_cols"])
            opts = []
            if fk['onDelete']: opts.append(f"onDelete: '{fk['onDelete']}'")
            if fk['onUpdate']: opts.append(f"onUpdate: '{fk['onUpdate']}'")
            opt = f", {{ {', '.join(opts)} }}" if opts else ''
            builders.append(
                f"fk_{'_'.join(fk['cols'])}_{fk['ref_table']}: "
                f"foreignKey({{ columns: [{cols}], foreignColumns: [{refs}] }}{opt})"
            )
    if builders:
        out.append(', (table) => ({')
        out += [f'  {b},' for b in builders]
        out.append('})')
    out.append(');')
    drizzle_ts = '\n'.join(out)

    # ── trigger DDL, if required ───────────────────────────────────────────────
    trigger_sql = ''
    if triggers_needed:
        fn = f'set_updated_at_{table_name}'
        trig = f'{table_name}_set_updated_at'
        trigger_sql = f"""\
CREATE OR REPLACE FUNCTION {fn}() RETURNS trigger AS $$
BEGIN
  NEW.updated_at := CURRENT_TIMESTAMP;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER {trig}
BEFORE UPDATE ON "{table_name}"
FOR EACH ROW
EXECUTE FUNCTION {fn}();
""".rstrip()

    return drizzle_ts, trigger_sql

def mysql_to_postgres_advanced(sql_text: str) -> str:
    """
    Best-effort converter from MySQL CREATE TABLE → Postgres DDL
    including:
      • audit columns forced to TIMESTAMP
      • AUTO_INCREMENT → GENERATED ALWAYS AS IDENTITY
      • removes MySQL-only clauses (ENGINE, CHARSET, ON UPDATE …)
    """
    s = (sql_text or '').strip()
    s = re.sub(r'`', '', s)

    # cut MySQL tail (ENGINE…)
    if (start := s.find('(')) != -1:
        depth = 0
        for i, ch in enumerate(s[start:], start):
            if ch == '(': depth += 1
            elif ch == ')':
                depth -= 1
                if depth == 0:
                    s = s[:i+1]
                    break
    # global cleans
    patterns = [r'\)\s*ENGINE\s*=\s*\w+.*?;?', r'DEFAULT\s+CHARSET\s*=\s*\w+',
                r'CHARSET\s*=\s*\w+', r'COLLATE\s*=\s*[\w_]+',
                r'ROW_FORMAT\s*=\s*\w+', r'COMMENT\s*=\s*\'[^\']*\'']
    for p in patterns:
        s = re.sub(p, '', s, flags=re.I)

    m = re.search(r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?([A-Za-z_]\w*)\s*\((.*)\)',
                  s, re.I | re.S)
    if not m:
        return s
    table, inner = m.group(1), m.group(2).strip()
    items = [x.strip() for x in re.split(r',(?![^()]*\))', inner) if x.strip()]

    def map_line(line: str) -> str:
        name = line.split()[0].lower() if line.split() else ''
        # audit cols → TIMESTAMP
        if name in ('created_at', 'updated_at', 'deleted_at'):
            line = re.sub(r'\b(?:DATETIME|TIMESTAMP|INT|BIGINT|CHAR|VARCHAR\(\d+\)|TEXT|DATE)\b',
                          'TIMESTAMP', line, flags=re.I)
        # remove ON UPDATE CURRENT_TIMESTAMP
        line = re.sub(r'ON\s+UPDATE\s+CURRENT_TIMESTAMP', '', line, flags=re.I)
        # core mappings
        subs = [
            (r'\bAUTO_INCREMENT\b', 'GENERATED ALWAYS AS IDENTITY'),
            (r'\bUNSIGNED\b', ''), (r'\bZEROFILL\b', ''),
            (r'\bTINYINT\s*\(\s*1\s*\)', 'BOOLEAN'),
            (r'\bTINYINT\b', 'SMALLINT'),
            (r'\bINT\s*\(\s*\d+\s*\)', 'INTEGER'),
            (r'\bINT\b', 'INTEGER'),
            (r'\bDATETIME\b', 'TIMESTAMP'),
            (r'\bLONGTEXT\b', 'TEXT'),
            (r'\bENUM\s*\([^)]+\)', 'TEXT'),
            (r'\bDOUBLE\s+PRECISION\b', 'DOUBLE PRECISION'),
            (r'\bDOUBLE\b', 'DOUBLE PRECISION'),
        ]
        for pat, repl in subs:
            line = re.sub(pat, repl, line, flags=re.I)
        return line.strip()

    body = ',\n  '.join(map_line(x) for x in items)
    return f'CREATE TABLE "{table}" (\n  {body}\n);'

@app.route("/api/drizzle", methods=["POST"])
def api_drizzle():
    data = request.get_json(silent=True) or {}
    sql = data.get("sql", "")
    try:
        code = sql_to_drizzle_advanced(sql)
        return jsonify({ "ok": True, "code": code })
    except Exception as e:
        return jsonify({ "ok": False, "error": str(e) }), 400

@app.route("/api/postgres", methods=["POST"])
def api_postgres():
    data = request.get_json(silent=True) or {}
    sql = data.get("sql", "")
    try:
        code = mysql_to_postgres_advanced(sql)
        return jsonify({ "ok": True, "sql": code })
    except Exception as e:
        return jsonify({ "ok": False, "error": str(e) }), 400
@app.route("/", methods=["GET"])
def index():
    return render_template("index.html")


if __name__ == "__main__":
    app.run(debug=True)
